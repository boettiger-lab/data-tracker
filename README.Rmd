---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# data-tracker

Proof of concept for Content-Identifier Based Registry for streaming data sources

## Goal

Data at a given URL may change periodically or even continuously.  This is particularly relevant with data
we might use for **forecasting**, such as environmental data from NOAA or NASA, or ecological data from NEON.
Such data sources rarely have DOIs, and it usually is not practical to mint a DOI for these sources every
time we make a forecast from them (see [why not just DOIs?](#DOIs?)). This repository outlines a simple
alternative approach.  


We want an automated job that:
- watches the URL
- computes a *identifier* for each unique copy of the data it finds (or is used in making a *forecast*)
- archives each new copy of the data that it finds
- Allows us to retrieve a the precise copy of that data using its *identifier*.


## Approach

Rather than using DOIs for this identifier, we will use simple content hash sums as identifiers, as proposed by Ben Trask, Jorrit Poelen, and others.  (Note that approach is different than that of `git`, `dat`, `IPFS` and other content-based systems in that it's way simpler -- no special software or complex protocol which generates "salted" hashes.  Our identifier is just the `sha256sum` of the raw data files.  A few simple helper utilities for doing this in R are provided in the experimental R package, [contenturi](https://github.com/cboettig/contenturi).  

## Automated Example Pipeline

To illustrate this, we'll consider the simple case of forecasting using the [classic Mauno Loa CO2 data](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html) (the longest record of direct measurements of CO2 in the atmosphere).  Weekly averages are published to  <ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_weekly_mlo.txt>, and as noted in the data file, data for the last several months [*may be subject to revision*](https://www.esrl.noaa.gov/gmd/ccgg/trends/trends_log.html) (hence we cannot necessarily get earlier versions of the actual data merely by dropping the latest rows from the latest version.)  This widely used and frequently updated dataset does not appear to have been assigned a DOI. Here, we grab weekly snapshots of the data and register permanent content-based identifiers.


This repository uses a scheduled CRON job on [GitHub Actions](https://github.com/boettiger-lab/data-tracker/actions/) to
store the content found at a URL of a possibly dynamic data resource.  


```{r}
library(contenturi) # remotes::install_github("cboettig/contenturi")
library(gert) # to push to github
```


First, we store a snapshot of the data to the local `store/` directory.  

```{r}
Sys.setenv("CONTENTURI_HOME" = "store/")
id <- store("ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_weekly_mlo.txt")
id
```

This is enough for local use. We can now access this specific data file by this content identifier:

```{r}
local_copy <- retrieve("hash://sha256/1dec4592238cb662f2afae13a8d49cb2caf90967e3b362e5cf868c3773db64f9")

```

And here it is:

```{r}
library(readr)
library(ggplot2)




co2 <- read_table2(local_copy, 
                ## data file doesn't have column names
                comment = "#",
                col_names = c("yr", "mo", "day", "decimal_date", "co2_ppm", 
                              "n_days", "yr_ago", "decade_ago", "since_1800"),
                na = c("-999.99", "-1")) 


ggplot(co2, aes(decimal_date, co2_ppm)) + geom_line()  
```


While we can access our data by this content identifier, so far our snapshot copy exists
only on our local machine. To make it a bit more accessible, let's publish it to a public
URL somewhere. For simplicity, we will merely push it to this GitHub repo: 

```{r}
library(gert)
git_add("store/")
git_commit_all("adding data")
git_push(password = Sys.getenv("secrets.GITHUB_TOKEN"))
```

Now that the data is on a public GitHub repo, we can access it by GitHub URL.  However, GitHub URLs aren't forever -- our repo could move or be deleted, GitHub Inc could shut down, etc. A GitHub URL (or any URL) makes a poor 'permanent identifier' for our content, so we don't want to hard-code that location.  But it does make a good practical location for the moment.  So, our trick is to use the content identifier in our code, but be able to "resolve" that identifier to the GitHub URL, much like we "resolve" a DOI to a landing page in some data repository.  At it's core, a DOI is just a redirect to another webpage.  

To allow our content identifier to act the same way, we create an entry in a 'content registry': a map of content identifiers to locations.  


Let's record those URLs in a local registry.

```{r}
url <- paste0("https://github.com/boettiger-lab/data-tracker/raw/master/",
                fs::path_rel(retrieve(id), "."))

id2 <- contenturi::register(url)
identical(id, id2)
```


The real power of this comes in when we register this information in other publicly queriable registeries, like <https://hash-archive.org>:

```{r}
contenturi::register(urls, "https://hash-archive.org")
```






The [Action](.github/workflows/rocker.yml) file has 4 steps:

1. `contenturi::store(url)`.  Caches the content to local `data/` directory and updates the local registry.
2. `git push` Commits the `data` dir and pushes it to GitHub, making this cache accessible at a public URL
3. `contenturi::register()`.  Register those GitHub URLs in both <https://hash-archive.org> and the local registry.
4. `git push` Commit and push the local `registry.tsv.gz`

Obviously this workflow could be adapted to publish the local content store and local registry somewhere else, (GitHub Release,
AWS S3 bucket, etc) rather than committing the data file directly to GitHub.  That is just a simple proof of principle. 


## Application

We can now query either the local or a remote registry like <hash-archive.org> by either the URL or the content identifier of
a specific version of interest, e.g.:

```{r}
library(contenturi)
```


```{r}
## look up by URL in the local registry
query("ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt", "data/")
```


```{r}
## look up by hash in the local & remote registries
query("hash://sha256/17b81c3c1c4a57e30371eaff008625f407116b38b3d679e547ac8fcbec73e1cb",
      c("https://hash-archive.org", "data/"))
```

Note that the query reports sightings of this content at the ftp address, our published git versions, and the local cache, each with timestamps.
