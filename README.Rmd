---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# data-tracker

Proof of concept for Content-Identifier Based Registry for streaming data sources

## Goal

Data at a given URL may change periodically or even continuously.  This is particularly relevant with data
we might use for **forecasting**, such as environmental data from NOAA or NASA, or ecological data from NEON.
Such data sources rarely have DOIs, and it usually is not practical to mint a DOI for these sources every
time we make a forecast from them (see [why not just DOIs?](#DOIs?)). This repository outlines a simple
alternative approach.  


We want an automated job that:
- watches the URL
- computes a *identifier* for each unique copy of the data it finds (or is used in making a *forecast*)
- archives each new copy of the data that it finds
- Allows us to retrieve a the precise copy of that data using its *identifier*.


## Approach

Rather than using DOIs for this identifier, we will use simple content hash sums as identifiers, as proposed by Ben Trask, Jorrit Poelen, and others.  (Note that approach is different than that of `git`, `dat`, `IPFS` and other content-based systems in that it's way simpler -- no special software or complex protocol which generates "salted" hashes.  Our identifier is just the `sha256sum` of the raw data files.  A few simple helper utilities for doing this in R are provided in the experimental R package, [contenturi](https://github.com/cboettig/contenturi).  

## Automated Example Pipeline

To illustrate this, we'll consider the simple case of forecasting using the [classic Mauno Loa CO2 data](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html) (the longest record of direct measurements of CO2 in the atmosphere).  Weekly averages are published to  <ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_weekly_mlo.txt>, and as noted in the data file, data for the last several months [*may be subject to revision*](https://www.esrl.noaa.gov/gmd/ccgg/trends/trends_log.html) (hence we cannot necessarily get earlier versions of the actual data merely by dropping the latest rows from the latest version.)  This widely used and frequently updated dataset does not appear to have been assigned a DOI. Here, we grab weekly snapshots of the data and register permanent content-based identifiers.


This repository uses a scheduled CRON job on [GitHub Actions](https://github.com/boettiger-lab/data-tracker/actions/) to
store the content found at a URL of a possibly dynamic data resource.  


```{r}
library(contenturi) # remotes::install_github("cboettig/contenturi")
library(gert) # to push to github
```


First, we store a snapshot of the data to the local `store/` directory.  

```{r}
id <- store("ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_weekly_mlo.txt", "store/")
id
```

This is enough for local use.  We can now refer to this data in our code:

```{r}
library(readr)
library(ggplot2)


co2 <- read_table2(retrieve(id, "store/"), 
                ## data file doesn't have column names
                comment = "#",
                col_names = c("yr", "mo", "day", "decimal_date", "co2_ppm", 
                              "n_days", "yr_ago", "decade_ago", "since_1800"),
                na = c("-999.99", "-1")) 


ggplot(co2, aes(decimal_date, co2_ppm)) + geom_line()  
```



Of course it is no good just having the data sitting in a local storage of some virtual machine that's about to dissappear.  We need to publish the store somewhere.  This could be anywhere we have a URL. 
For simplicity, we will merely push it to GitHub. 

```{r}
library(gert)
git_add("store/")
git_commit_all("adding data", author = Sys.getenv("github.actor"))
git_push()
```

```{r}
git_push(paste0("https://", 
                Sys.getenv("github.actor"), ":", 
                Sys.getenv("secrets.GITHUB_TOKEN"),
                "@github.com/",
                Sys.getenv("github.repository"),
                ".git HEAD:",
                Sys.getenv("github.ref")
                ))
```

The data is on a public GitHub repo, so we can access it by URL.  Let's record those URLs in a local registry.

```{r}
urls <- paste0("https://github.com/boettiger-lab/data-tracker/raw/master/",
               list.files("store", full.names = TRUE, recursive = TRUE))

contenturi::register(urls, "store/")
```


The real power of this comes in when we register this information in other publicly queriable registeries, like <https://hash-archive.org>:

```{r}
contenturi::register(urls, "https://hash-archive.org")
```






The [Action](.github/workflows/rocker.yml) file has 4 steps:

1. `contenturi::store(url)`.  Caches the content to local `data/` directory and updates the local registry.
2. `git push` Commits the `data` dir and pushes it to GitHub, making this cache accessible at a public URL
3. `contenturi::register()`.  Register those GitHub URLs in both <https://hash-archive.org> and the local registry.
4. `git push` Commit and push the local `registry.tsv.gz`

Obviously this workflow could be adapted to publish the local content store and local registry somewhere else, (GitHub Release,
AWS S3 bucket, etc) rather than committing the data file directly to GitHub.  That is just a simple proof of principle. 


## Application

We can now query either the local or a remote registry like <hash-archive.org> by either the URL or the content identifier of
a specific version of interest, e.g.:

```{r}
library(contenturi)
```


```{r}
## look up by URL in the local registry
query("ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt", "data/")
```


```{r}
## look up by hash in the local & remote registries
query("hash://sha256/17b81c3c1c4a57e30371eaff008625f407116b38b3d679e547ac8fcbec73e1cb",
      c("https://hash-archive.org", "data/"))
```

Note that the query reports sightings of this content at the ftp address, our published git versions, and the local cache, each with timestamps.
